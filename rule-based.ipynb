{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7834bd6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import re\n",
    "\n",
    "import math\n",
    "import torch\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import dill\n",
    "import random\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a90f9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rule_based_utils import *\n",
    "from chordify_json_extension import all_chords, check_chordify, gram2chordify_chord, convert2chordify\n",
    "from thresholding_and_grouping import *\n",
    "from rule_based import give_subset, get_uni_gram, train_all, RuleModel, evaluate_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ad2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2extract = ['song','CFP','CFD',\n",
    "                'UC','RHC','CPT',\n",
    "                'BD','R','weighted_total']\n",
    "\n",
    "\n",
    "files_and_annotations = extract_chord_path_and_annotations(\"Annotations.csv\", cols2extract, delimiter=\",\")\n",
    "chords_and_annotations = get_chords_and_annotations(files_and_annotations, no_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3edb94d",
   "metadata": {},
   "source": [
    "## get all uni, bi, tri, and quad grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e29526",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_uni_gram, all_bi_gram, all_tri_gram, all_quad_gram = {},{},{},{}\n",
    "for (chords, _, sub_scores, total) in chords_and_annotations:\n",
    "    long_string = process_song(chords,  remove_nl=True)\n",
    "    if long_string.find('||') != -1:\n",
    "        print(chords, '\\n\\n', long_string)\n",
    "        break\n",
    "    new_uni, new_bi, new_tri, new_quad = extract_n_grams(long_string)\n",
    "    all_uni_gram = update_dict(new_uni, all_uni_gram)\n",
    "    all_bi_gram = update_dict(new_bi, all_bi_gram)\n",
    "    all_tri_gram = update_dict(new_tri, all_tri_gram)\n",
    "    all_quad_gram = update_dict(new_quad, all_quad_gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd79eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_uni_gram_keys= simplify_grams(all_uni_gram)\n",
    "stripped_bi_gram_keys= simplify_grams(all_bi_gram)\n",
    "stripped_tri_gram_keys= simplify_grams(all_tri_gram)        \n",
    "stripped_quad_gram_keys= simplify_grams(all_quad_gram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17642693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all grams without simplification:\n",
      "545 2722 4992 6842\n",
      "all grams simplified (removing all () and / )\n",
      "226 1946 4187 6175\n"
     ]
    }
   ],
   "source": [
    "print(\"all grams without simplification:\")\n",
    "print(len(all_uni_gram),len(all_bi_gram),len(all_tri_gram),len(all_quad_gram),)\n",
    "print(\"all grams simplified (removing all () and / )\")\n",
    "print(len(stripped_uni_gram_keys),len(stripped_bi_gram_keys),len(stripped_tri_gram_keys),len(stripped_quad_gram_keys),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270a14b",
   "metadata": {},
   "source": [
    "### Compute IDF dicts for original and simplified grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c48e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_idf_dict = get_idf_dict(chords_and_annotations,n_gram=1,simplified=False)\n",
    "all_idf_dict = {k:math.log(200/v) for k,v in all_idf_dict.items()}\n",
    "\n",
    "simplified_idf_dict = get_idf_dict(chords_and_annotations,n_gram=1,simplified=True)\n",
    "simplified_idf_dict = {k:math.log(200/v) for k,v in simplified_idf_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f26542",
   "metadata": {},
   "source": [
    "### score uncommonness of chords in three boundary threshold finding ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2417bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_chords.keys()) == 3721, \"somethign wrong should have 3721 keys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db2db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplified: 'B:aug/5' to 'B:aug'\n",
      "simplified: 'D:min9/5' to 'D:min9'\n",
      "simplified: 'Cb:maj/5' to 'Cb:maj'\n",
      "simplified: 'Eb:min/b7' to 'Eb:min'\n",
      "simplified: 'Gb:maj7/2' to 'Gb:maj7'\n",
      "simplified: 'F:9/5' to 'F:9'\n",
      "simplified: 'A:5/6' to 'A:5'\n"
     ]
    }
   ],
   "source": [
    "# check in uni_gram chords are in chordify guitargram dict\n",
    "for keys in all_uni_gram.keys():\n",
    "    if keys[0].find('/') != -1 and keys[0].find('(') == -1:\n",
    "        conv, simpl = convert2chordify(keys[0], simplify=True)\n",
    "        if not check_chordify(conv, print_all=False):\n",
    "            print(f\"not in chordify: {keys[0]} converted to: {conv}\")\n",
    "        if simpl:\n",
    "            print(f\"simplified: '{keys[0]}' to '{conv}'\")\n",
    "#         print(check_chordify(conv, print_all=False), simpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7028e63",
   "metadata": {},
   "source": [
    "## Generate 10 random splits for the k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s= list(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86447653",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(s)\n",
    "split_size = 20\n",
    "split_1_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_2_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_3_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_4_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_5_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_6_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_7_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_8_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_9_ind = s[-split_size:]\n",
    "del s[-split_size:]\n",
    "split_10_ind = s[-split_size:]\n",
    "del s[-split_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965ddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_1_ind)\n",
    "print(split_2_ind)\n",
    "print(split_3_ind)\n",
    "print(split_4_ind)\n",
    "print(split_5_ind)\n",
    "print(split_6_ind)\n",
    "print(split_7_ind)\n",
    "print(split_8_ind)\n",
    "print(split_9_ind)\n",
    "print(split_10_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8fc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## final indices\n",
    "split_1_ind =[88, 71, 4, 171, 33, 56, 105, 72, 139, 64, 26, 117, 151, 183, 80, 153, 121, 63, 176, 97]\n",
    "split_2_ind =[132, 182, 3, 168, 39, 165, 147, 109, 114, 0, 177, 133, 160, 198, 20, 75, 32, 123, 157, 193]\n",
    "split_3_ind =[167, 57, 100, 195, 27, 118, 128, 190, 103, 141, 162, 186, 13, 81, 55, 8, 36, 135, 50, 2]\n",
    "split_4_ind =[172, 79, 10, 148, 73, 92, 164, 34, 40, 47, 11, 25, 98, 85, 136, 5, 142, 99, 161, 126]\n",
    "split_5_ind =[102, 62, 23, 74, 178, 170, 112, 24, 106, 95, 152, 90, 101, 138, 21, 197, 7, 150, 77, 42]\n",
    "split_6_ind =[35, 187, 52, 194, 115, 16, 116, 6, 53, 45, 15, 67, 131, 185, 191, 1, 96, 28, 19, 66]\n",
    "split_7_ind =[83, 9, 59, 175, 76, 22, 51, 69, 14, 94, 38, 46, 154, 91, 104, 173, 93, 108, 48, 12]\n",
    "split_8_ind =[180, 86, 137, 119, 43, 120, 84, 49, 58, 18, 89, 179, 155, 166, 111, 129, 192, 143, 134, 124]\n",
    "split_9_ind =[17, 159, 82, 61, 41, 54, 184, 130, 199, 188, 181, 78, 60, 169, 174, 37, 29, 140, 145, 163]\n",
    "split_10_ind =[144, 87, 70, 127, 68, 44, 196, 158, 31, 125, 113, 146, 122, 65, 156, 30, 149, 189, 107, 110]\n",
    "split_dict = {\n",
    "    1: split_1_ind,\n",
    "    2: split_2_ind,\n",
    "    3: split_3_ind,\n",
    "    4: split_4_ind,\n",
    "    5: split_5_ind,\n",
    "    6: split_6_ind,\n",
    "    7: split_7_ind,\n",
    "    8: split_8_ind,\n",
    "    9: split_9_ind,\n",
    "    0: split_10_ind,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdbcdd",
   "metadata": {},
   "source": [
    "### Do k-fold cross validation for rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ced89",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_performance = []\n",
    "for i in range(10):\n",
    "    # obtain train-validate-test-splits\n",
    "    indices = np.asarray(list(range(10))) + i\n",
    "    \n",
    "    train_ind = [split_dict[(m% 10)] for m in indices[:-1]]\n",
    "    validate_ind = [split_dict[(m % 10)] for m in indices[-1:]]\n",
    "\n",
    "    train_ind = [k for j in train_ind for k in j]\n",
    "    validate_ind = [k for j in validate_ind for k in j]\n",
    "    \n",
    "    # obtain chords, lab_files and rest of data\n",
    "    train = give_subset(train_ind, chords_and_annotations)\n",
    "    validate = give_subset(validate_ind, chords_and_annotations)\n",
    "    \n",
    "    # obtain idf and gram dicts\n",
    "    train_idf_dict = get_idf_dict(train, n_gram=1, simplified=False)\n",
    "    train_idf_dict = {k:math.log(200/v) for k,v in train_idf_dict.items()}\n",
    "    train_idf_dict = defaultdict(lambda:max(train_idf_dict.values()),train_idf_dict)\n",
    "    train_uni_grams = get_uni_gram(train)\n",
    "    \n",
    "    # \"train\" rule based model\n",
    "    cfg = train_all(train, train_idf_dict, train_uni_grams)\n",
    "    \n",
    "    # create trained model\n",
    "    kfold_model = RuleModel(cfg, train_uni_grams,  train_idf_dict)\n",
    "    \n",
    "    # evaluate for \n",
    "    all_performance = evaluate_test_set(kfold_model, validate, classify=False, custom_loss=True)\n",
    "    save_performance.append(all_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89571e51",
   "metadata": {},
   "source": [
    "## Do k-fold cross validation for length-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_len(chords_and_annos, only_len=False, type_of_len='char'):\n",
    "    cat_scores = {\n",
    "        1: {1: [], 2: [], 3: [], 4: []},\n",
    "        2: {1: [], 2: [], 3: [], 4: []},\n",
    "        3: {1: [], 2: [], 3: [], 4: []},\n",
    "        4: {1: [], 2: [], 3: [], 4: []},\n",
    "        5: {1: [], 2: [], 3: [], 4: []},\n",
    "        6: {1: [], 2: [], 3: [], 4: []},\n",
    "        7: {1: [], 2: [], 3: [], 4: []},\n",
    "    }\n",
    "    weigthed_uni_gram_per_song = []\n",
    "\n",
    "    lens = []\n",
    "    \n",
    "    for (chords, lab_file, sub_scores, total) in chords_and_annos:\n",
    "        total_chord_count, song_score = 0, 0\n",
    "        \n",
    "        long_string = process_song(chords)\n",
    "        long_string_nl = process_song(chords, remove_nl=False)\n",
    "        annotations = long_string_nl.replace('\\n', ' \\n ').replace('  ', ' ').replace('||', '|')\n",
    "        if type_of_len == 'char':\n",
    "            cat_scores = len(annotations)\n",
    "        elif type_of_len == 'dotsplit':\n",
    "            new_anno = []\n",
    "            for cho in annotations.split(' '):\n",
    "                if cho.find(':') != -1:\n",
    "                    new_anno.append(cho.split(':')[0])\n",
    "                    new_anno.append(cho.split(':')[1])\n",
    "                else:\n",
    "                    new_anno.append(cho)\n",
    "            cat_scores = len(new_anno)\n",
    "        elif type_of_len == 'guitar':\n",
    "            annotations = annotations.split(' ')\n",
    "            try:\n",
    "                annotations.remove('')\n",
    "            except:\n",
    "                pass\n",
    "            new_anno = ''\n",
    "            for ann in annotations:\n",
    "                if ann in ['|', '\\n', 'N', '*']:\n",
    "                    new_anno += ann + ' '\n",
    "                else:\n",
    "                    converted = convert2chordify(ann)\n",
    "                    to_add = all_chords[converted[0]]['guitar']\n",
    "                    new_anno += str(to_add)\n",
    "            cat_scores= len(new_anno)\n",
    "\n",
    "        if only_len:\n",
    "            lens.append(cat_scores)\n",
    "\n",
    "        for i in range(1, 8):\n",
    "            cat_scores[i][sub_scores[i-1]].append(cat_scores)  \n",
    "    \n",
    "    if only_len:\n",
    "        return lens\n",
    "    return cat_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60031118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46bcdfa4",
   "metadata": {},
   "source": [
    "### extracting encoding dict for : split chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_chars = []\n",
    "give_subset()\n",
    "for chords, _, _, _ in chords_and_annotations:\n",
    "    big_string = new_big_string(chords, remove_nl=False)\n",
    "    for cho in big_string.split(' '):\n",
    "        if cho.find(':') != -1:\n",
    "            possible_chars.append(cho.split(':')[0])\n",
    "            possible_chars.append(cho.split(':')[1])\n",
    "        else:\n",
    "            possible_chars.append(cho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be600e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict = {k:i+2 for i,k in enumerate(set(possible_chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict['unk'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cce9f",
   "metadata": {},
   "source": [
    "### create new snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b55926",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d30db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aad005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stuff():\n",
    "    sub_chords = []\n",
    "    for i in range(10):\n",
    "        # obtain train-validate-test-splits\n",
    "        indices = np.asarray(list(range(10))) + i\n",
    "        train_ind = [split_dict[(m% 10)] for m in indices[:-2]]\n",
    "    #     validate_ind = [split_dict[(m % 10)] for m in indices[-4:-2]]\n",
    "        test_ind = [split_dict[(m % 10)] for m in indices[-2:]]\n",
    "\n",
    "        train_ind = [k for j in train_ind for k in j]\n",
    "    #     validate_ind = [k for j in validate_ind for k in j]\n",
    "        test_ind = [k for j in test_ind for k in j]\n",
    "\n",
    "        # obtain chords, lab_files and rest of data\n",
    "        train = give_subset(train_ind, chords_and_annotations)\n",
    "    #     validate = give_subset(validate_ind, chords_and_annotations)\n",
    "        test = give_subset(test_ind, chords_and_annotations)\n",
    "\n",
    "        # obtain idf and gram dicts\n",
    "        train_idf_dict = get_idf_dict(train, n_gram=1, simplified=False)\n",
    "        train_idf_dict = {k:math.log(200/v) for k,v in train_idf_dict.items()}\n",
    "\n",
    "        train_idf_dict = defaultdict(lambda:max(train_idf_dict.values()),train_idf_dict)\n",
    "        train_uni_grams = get_uni_gram(train)\n",
    "\n",
    "        # \"train\" rule based model\n",
    "        cfg = train_all(train, train_idf_dict, train_uni_grams)\n",
    "\n",
    "        # create trained model\n",
    "        kfold_model = RuleModel(cfg, train_uni_grams,  train_idf_dict)\n",
    "        with open(f'./rule_models/fold_{i}.pkl','wb') as f:\n",
    "            dill.dump(kfold_model, f)\n",
    "        with open(f'./rule_models/idf_dict_{i}.pkl','wb') as f:\n",
    "            dill.dump(kfold_model.idf_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e32e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_stuff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51545431",
   "metadata": {},
   "outputs": [],
   "source": [
    "if x:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda976ed",
   "metadata": {},
   "source": [
    "## Classify custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data = pd.read_csv(\"chords_with_transpose_and_difficulties.csv\", delimiter='\\t')\n",
    "file_with_added_scores = \"chords_with_transpose_and_difficulties_and_playability.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cad15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter2num = {\n",
    "    'B': 0,\n",
    "    'C': 0,\n",
    "    'C#': 1,\n",
    "    'Db': 1,\n",
    "    'D': 2,\n",
    "    'D#': 3,\n",
    "    'Eb': 3,\n",
    "    'E': 4,\n",
    "    'E#': 5,\n",
    "    'Fb': 4,\n",
    "    'F': 5,\n",
    "    'F#': 6,\n",
    "    'Gb': 6,\n",
    "    'G': 7,\n",
    "    'G#': 8,\n",
    "    'Ab': 8,\n",
    "    'A': 9,\n",
    "    'A#': 10,\n",
    "    'Bb': 10,\n",
    "    'B': 11,\n",
    "    'Cb': 11,\n",
    "}\n",
    "num2letter = {\n",
    "    0 : 'C',\n",
    "    1 : 'C#',\n",
    "    2 : 'D',\n",
    "    3 : 'Eb',\n",
    "    4 : 'E',\n",
    "    5 : 'F',\n",
    "    6 : 'F#',\n",
    "    7 : 'G',\n",
    "    8 : 'Ab',\n",
    "    9 : 'A',\n",
    "    10 : 'Bb',\n",
    "    11 : 'B',\n",
    "}\n",
    "def magic(chord, transposition):\n",
    "    if chord == 'N':\n",
    "        return chord\n",
    "    root, rest = chord.split(':')\n",
    "    root_number = letter2num[root]\n",
    "    root_transposed = (root_number + transposition) % 12\n",
    "    root_transposed = num2letter[root_transposed]\n",
    "    return root_transposed + ':' + rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630aad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lab_seq(chordify_data, transposition):\n",
    "    start = True\n",
    "    phrase_counter = 0\n",
    "    cleaned_sequence = ''\n",
    "    lab_sequence = ''\n",
    "    prev = 0\n",
    "    cur = 0\n",
    "    for chord in chordify_data:\n",
    "        chord_info = chord.split(';')\n",
    "        prev = cur\n",
    "        cur = int(chord_info[0])\n",
    "        if start:\n",
    "            start = False\n",
    "            cleaned_sequence += f'{chord_info[2]}\\t| '\n",
    "        if cur < prev:\n",
    "            if phrase_counter >= 4:\n",
    "                cleaned_sequence += f'|\\nPHRASE{chord_info[2]}\\t'\n",
    "                phrase_counter = 0\n",
    "            cleaned_sequence += '| '\n",
    "            phrase_counter += 1\n",
    "                \n",
    "\n",
    "        cleaned_sequence += magic(chord_info[1],transposition) + ' '\n",
    "\n",
    "        lab_sequence += chord_info[2] + '\\t' + chord_info[3] + '\\t' + chord_info[1] +'\\n'\n",
    "    cleaned_sequence += '|'\n",
    "    return cleaned_sequence, lab_sequence\n",
    "    \n",
    "def rule_based_custom_data(seq_data, lab_data):\n",
    "    models = []\n",
    "    all_songs_classified = []\n",
    "    for i in range(10):\n",
    "        with open(f'./rule_models/fold_{i}.pkl', 'rb') as f:\n",
    "            loaded_model = dill.load(f)\n",
    "        with open(f'./rule_models/idf_dict_{i}.pkl','rb') as f:\n",
    "            load_model_cat_dict = dill.load(f)\n",
    "        loaded_model.idf_dict = load_model_cat_dict\n",
    "        models.append(loaded_model)\n",
    "                \n",
    "    for chord, lab in zip(seq_data,lab_data):\n",
    "        song_stats = []\n",
    "        for model in models:\n",
    "            song_stats.append(model.predict(chord, lab))\n",
    "#             print(model.predict(chord, lab))\n",
    "        all_songs_classified.append(song_stats)\n",
    "#         print()\n",
    "    return all_songs_classified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d23aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_seq = []\n",
    "only_lab = []\n",
    "for i, row in custom_data[[\"chords\", \"transposed_amount\"]].iterrows():\n",
    "    better_chords = row[\"chords\"].split('\\\\n')\n",
    "    better_chords.remove('')\n",
    "    x = extract_lab_seq(better_chords, row[\"transposed_amount\"])\n",
    "    only_seq.append(x[0].split('PHRASE'))\n",
    "    only_lab.append(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a06e4",
   "metadata": {},
   "source": [
    "## generate lstm playability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88042e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import retrieve\n",
    "from models import LSTMBase\n",
    "from rule_based_utils import new_big_string\n",
    "data = retrieve('billboard_salami')\n",
    "custom = retrieve('custom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60db8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only to extract encoding dict for trained models\n",
    "for p in ['CFP']:#,'CFD','UC','RHC','CPT','BD','R','weighted_total']:\n",
    "    all_data = data(\n",
    "        seq_len=-1,#config['seq_len'],\n",
    "        batch_size=2,#config['batch_size'],\n",
    "        num_workers=0,#config['num_workers'],\n",
    "        encoding='char',#config['encoding'],\n",
    "        target=p,\n",
    "        colla=True, #config['colla'],\n",
    "        target_data_path='./Annotations_with_locations.csv'\n",
    "\n",
    "    )\n",
    "    all_data.custom_setup(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267645e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_loader = custom(only_seq2_model_input, encode_dict = all_data.char_dict, encode_dict_size= all_data.char_dict_size)\n",
    "train_loader = custom_loader.loader(batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b3ae3",
   "metadata": {},
   "source": [
    "## add rule-based scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_outputs = rule_based_custom_data(only_seq, only_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2837bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all_1, all_all_2, all_all_3, all_all_4, all_all_5, all_all_6, all_all_7, all_all  = [],[],[],[],[],[],[],[]\n",
    "cur_all_1 ,cur_all_2 ,cur_all_3 ,cur_all_4 ,cur_all_5 ,cur_all_6 ,cur_all_7 , cur_all = [],[],[],[],[],[],[],[]\n",
    "for index, songies in enumerate(all_the_outputs):\n",
    "    if index % 12 == 0:\n",
    "        print(cur_all)#, np.argsort(cur_all))\n",
    "        all_all_1.append(cur_all_1)\n",
    "        all_all_2.append(cur_all_2)\n",
    "        all_all_3.append(cur_all_3)\n",
    "        all_all_4.append(cur_all_4)\n",
    "        all_all_5.append(cur_all_5)\n",
    "        all_all_6.append(cur_all_6)\n",
    "        all_all_7.append(cur_all_7)\n",
    "        all_all.append(cur_all)\n",
    "        cur_all_1 ,cur_all_2 ,cur_all_3 ,cur_all_4 ,cur_all_5 ,cur_all_6 ,cur_all_7 , cur_all = [],[],[],[],[],[],[],[]\n",
    "#         print()\n",
    "    avg = [sum(y) / len(y) for y in zip(*songies)]\n",
    "    cur_all_1.append(round(avg[0]/3, 4))\n",
    "    cur_all_2.append(round(avg[1]/3, 4))\n",
    "    cur_all_3.append(round(avg[2]/3, 4))\n",
    "    cur_all_4.append(round(avg[3]/3, 4))\n",
    "    cur_all_5.append(round(avg[4]/3, 4))\n",
    "    cur_all_6.append(round(avg[5]/3, 4))\n",
    "    cur_all_7.append(round(avg[6]/3, 4))\n",
    "    cur_all.append(round(avg[7]/40, 4))\n",
    "\n",
    "all_all_1.append(cur_all_1)\n",
    "all_all_2.append(cur_all_2)\n",
    "all_all_3.append(cur_all_3)\n",
    "all_all_4.append(cur_all_4)\n",
    "all_all_5.append(cur_all_5)\n",
    "all_all_6.append(cur_all_6)\n",
    "all_all_7.append(cur_all_7)\n",
    "all_all.append(cur_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6575243",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unzipped_all_al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0115f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "unzipped_all_al_1 = [y for x in all_all_1 for y in x]\n",
    "unzipped_all_al_2 = [y for x in all_all_2 for y in x]\n",
    "unzipped_all_al_3 = [y for x in all_all_3 for y in x]\n",
    "unzipped_all_al_4 = [y for x in all_all_4 for y in x]\n",
    "unzipped_all_al_5 = [y for x in all_all_5 for y in x]\n",
    "unzipped_all_al_6 = [y for x in all_all_6 for y in x]\n",
    "unzipped_all_al_7 = [y for x in all_all_7 for y in x] \n",
    "unzipped_all_al = [y for x in all_all for y in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad9dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_rule_based['RB_CFP'] = unzipped_all_al_1\n",
    "custom_data_rule_based['RB_CFD'] = unzipped_all_al_2\n",
    "custom_data_rule_based['RB_UC'] = unzipped_all_al_3\n",
    "custom_data_rule_based['RB_RHC'] = unzipped_all_al_4\n",
    "custom_data_rule_based['RB_CPT'] = unzipped_all_al_5\n",
    "custom_data_rule_based['RB_BD'] = unzipped_all_al_6\n",
    "custom_data_rule_based['RB_R'] = unzipped_all_al_7\n",
    "custom_data_rule_based['RB_weighted_total'] = unzipped_all_al\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51042d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in added_cols:\n",
    "    custom_data[col] = all_thingies[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a430e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data.to_csv(file_with_added_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3981791",
   "metadata": {},
   "source": [
    "## adding ML playability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162269fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, term in enumerate(['CFP','CFD','UC','RHC','CPT','BD','R','weighted_total']):\n",
    "    custom_data_rule_based[term] = ml_predicted_cat_playability[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_scores_nrows_10000.pkl\", 'rb') as f:\n",
    "    playability0 = pkl.load(f)\n",
    "with open(\"all_scores_10000_20000.pkl\", 'rb') as f:\n",
    "    playability1 = pkl.load(f)\n",
    "with open(\"all_scores_20000_30000.pkl\", 'rb') as f:\n",
    "    playability2 = pkl.load(f)\n",
    "with open(\"all_scores_30000_40000.pkl\", 'rb') as f:\n",
    "    playability3 = pkl.load(f)\n",
    "with open(\"all_scores_40000_50000.pkl\", 'rb') as f:\n",
    "    playability4 = pkl.load(f)\n",
    "with open(\"all_scores_50000_60000.pkl\", 'rb') as f:\n",
    "    playability5 = pkl.load(f)\n",
    "with open(\"all_scores_60000_70000.pkl\", 'rb') as f:\n",
    "    playability6 = pkl.load(f)\n",
    "with open(\"all_scores_70000_80000.pkl\", 'rb') as f:\n",
    "    playability7 = pkl.load(f)\n",
    "with open(\"all_scores_80000_90000.pkl\", 'rb') as f:\n",
    "    playability8 = pkl.load(f)\n",
    "with open(\"all_scores_90000_100000.pkl\", 'rb') as f:\n",
    "    playability9 = pkl.load(f)\n",
    "with open(\"all_scores_nrows_10000_last_cat.pkl\", 'rb') as f:\n",
    "    playability10 = pkl.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_thingies = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
    "for i in range(8):\n",
    "    all_thingies[i] += playability0[i] +playability1[i] +playability2[i] +playability3[i] +playability4[i] +playability5[i] +playability6[i] +playability7[i] +playability8[i] +playability9[i] + playability10[i]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dotsplit_all_scores_nrows_10000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability0 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_10000_20000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability1 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_20000_30000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability2 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_30000_40000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability3 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_40000_50000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability4 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_50000_60000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability5 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_60000_70000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability6 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_70000_80000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability7 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_80000_90000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability8 = pkl.load(f)\n",
    "with open(\"dotsplit_all_scores_90000_100000.pkl\", 'rb') as f:\n",
    "    dotsplit_playability9 = pkl.load(f)\n",
    "    \n",
    "dotsplit_all_thingies = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[]}\n",
    "for i in range(8):\n",
    "    dotsplit_all_thingies[i] += dotsplit_playability0[i] + dotsplit_playability1[i] + dotsplit_playability2[i] + dotsplit_playability3[i] + dotsplit_playability4[i] + dotsplit_playability5[i] + dotsplit_playability6[i] + dotsplit_playability7[i] + dotsplit_playability8[i] + dotsplit_playability9[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    dotsplit_all_thingies[i] = np.asarray(dotsplit_all_thingies[i]) /3 if i != 7 else np.asarray(dotsplit_all_thingies[i]) /39\n",
    "    dotsplit_all_thingies[i] = dotsplit_all_thingies[i].round(4)\n",
    "    print(np.asarray(dotsplit_all_thingies[i]).mean(), np.asarray(dotsplit_all_thingies[i]).var(), np.asarray(dotsplit_all_thingies[i]).min(), np.asarray(dotsplit_all_thingies[i]).max()) \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotsplit_all_thingies[i] = np.asarray(dotsplit_all_thingies[i]) /3 if i != 7 else np.asarray(dotsplit_all_thingies[i]) /39\n",
    "dotsplit_all_thingies[i] = dotsplit_all_thingies[i].round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627781ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_cols= ['GRU_char_CFP','GRU_char_CFD','GRU_char_UC','GRU_char_RHC','GRU_char_CPT','GRU_char_BD','GRU_char_R','GRU_char_weighted_total']\n",
    "\n",
    "added_dotsplit_cols= ['GRU_dotsplit_CFP','GRU_dotsplit_CFD','GRU_dotsplit_UC','GRU_dotsplit_RHC','GRU_dotsplit_CPT','GRU_dotsplit_BD','GRU_dotsplit_R','GRU_dotsplit_weighted_total']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, col in enumerate(added_cols):\n",
    "#     all_thingies[col] = all_thingies[i]\n",
    "#     del all_thingies[i]\n",
    "for i, col in enumerate(added_dotsplit_cols):\n",
    "    dotsplit_all_thingies[col] = dotsplit_all_thingies[i]\n",
    "    del dotsplit_all_thingies[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6de33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in added_dotsplit_cols:\n",
    "    print(col)\n",
    "    custom_data_rule_based[col] = dotsplit_all_thingies[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_rule_based.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e0004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in added_cols:\n",
    "    custom_data_rule_based[col] = all_thingies[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_rule_based.to_csv(\"chords_with_transpose_difficulties_and_playability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02483b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_rule_based[['difficulty', 'GRU_char_weighted_total','GRU_dotsplit_weighted_total','RB_weighted_total']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
